{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9787669,"sourceType":"datasetVersion","datasetId":5997071}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scikit-learn pandas numpy torch","metadata":{"id":"HCChIPLIHGZs","outputId":"7b5721b5-371f-432a-bb35-157ae8da24bf","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T19:46:58.798251Z","iopub.execute_input":"2024-11-04T19:46:58.798650Z","iopub.status.idle":"2024-11-04T19:47:11.183150Z","shell.execute_reply.started":"2024-11-04T19:46:58.798611Z","shell.execute_reply":"2024-11-04T19:47:11.182202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport math\n\nfrom typing import List, Tuple, Dict\n\n\n\n\n\nclass TimeSeriesDataset(Dataset):\n\n    def __init__(self, data_path: str, index_path: str, seq_length: int = 50):\n\n        #print(f\"Loading data from {data_path} and {index_path}\")\n\n\n\n        self.data = pd.read_csv(f'/kaggle/input/twitter/{data_path}')\n\n        self.index = pd.read_csv(f'/kaggle/input/twitter/{index_path}')\n\n\n\n        print(f\"Data shape: {self.data.shape}\")\n\n        print(f\"Index shape: {self.index.shape}\")\n\n        print(f\"Index columns: {self.index.columns.tolist()}\")\n\n        print(f\"Start index range: {self.index['start_ind'].min()} to {self.index['start_ind'].max()}\")\n\n        print(f\"End index range: {self.index['end_ind'].min()} to {self.index['end_ind'].max()}\")\n\n\n\n        # Normalize indices to match data length\n\n        max_idx = len(self.data) - 1\n\n        self.index['start_ind'] = self.index['start_ind'].apply(\n\n            lambda x: int(x % (max_idx + 1)) if pd.notnull(x) else 0\n\n        )\n\n        self.index['end_ind'] = self.index['end_ind'].apply(\n\n            lambda x: int(x % (max_idx + 1)) if pd.notnull(x) else max_idx\n\n        )\n\n\n\n        print(\"\\nAfter normalization:\")\n\n        print(f\"Start index range: {self.index['start_ind'].min()} to {self.index['start_ind'].max()}\")\n\n        print(f\"End index range: {self.index['end_ind'].min()} to {self.index['end_ind'].max()}\")\n\n\n\n        self.seq_length = seq_length\n\n        self.time_scaler = MinMaxScaler()\n\n        self.follower_scaler = StandardScaler()\n\n\n\n        # Preprocess\n\n        self.preprocess_data()\n\n        self.cascade_sequences = self.create_sequences()\n\n        print(f\"\\nCreated {len(self.cascade_sequences)} valid sequences\")\n\n\n\n        if len(self.cascade_sequences) == 0:\n\n            raise ValueError(\"No valid sequences were created. Check your data and index files.\")\n\n\n\n    def preprocess_data(self):\n\n        print(\"\\nPreprocessing data...\")\n\n        # Check if required columns exist\n\n        required_cols = ['number_of_followers', 'relative_time_second']\n\n        missing_cols = [col for col in required_cols if col not in self.data.columns]\n\n        if missing_cols:\n\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n\n\n\n        self.data['log_followers'] = np.log1p(self.data['number_of_followers'])\n\n        self.data['normalized_time'] = self.time_scaler.fit_transform(\n\n            self.data['relative_time_second'].values.reshape(-1, 1)\n\n        )\n\n        self.data['normalized_followers'] = self.follower_scaler.fit_transform(\n\n            self.data['log_followers'].values.reshape(-1, 1)\n\n        )\n\n        print(\"Data preprocessing completed\")\n\n\n\n    def create_sequences(self) -> List[Tuple[np.ndarray, int]]:\n\n        print(\"\\nCreating sequences...\")\n\n        sequences = []\n\n        total_rows = len(self.index)\n\n        valid_sequences = 0\n\n        skipped_sequences = 0\n\n\n\n        for idx, row in self.index.iterrows():\n\n            try:\n\n                start_idx = int(row['start_ind'])\n\n                end_idx = int(row['end_ind'])\n\n\n\n                # Validate sequence bounds\n\n                if start_idx > end_idx:\n\n                    start_idx, end_idx = end_idx, start_idx\n\n\n\n                if not (0 <= start_idx < len(self.data) and 0 <= end_idx < len(self.data)):\n\n                    skipped_sequences += 1\n\n                    if idx % 1000 == 0:\n\n                        print(f\"Skipping sequence at index {idx}: Invalid bounds ({start_idx}, {end_idx})\")\n\n                    continue\n\n\n\n                cascade = self.data.iloc[start_idx:end_idx + 1]\n\n                if len(cascade) >= self.seq_length:\n\n                    seq_data = cascade[['normalized_time', 'normalized_followers']].values\n\n                    sequences.append((seq_data, len(cascade)))\n\n                    valid_sequences += 1\n\n                else:\n\n                    skipped_sequences += 1\n\n            except Exception as e:\n\n                print(f\"Error processing sequence at index {idx}: {e}\")\n\n                skipped_sequences += 1\n\n                continue\n\n\n\n            if idx % 1000 == 0:\n\n                print(f\"Processed {idx+1}/{total_rows} sequences. \"\n\n                      f\"Valid: {valid_sequences}, Skipped: {skipped_sequences}\")\n\n\n\n        print(f\"\\nSequence creation completed:\")\n\n        print(f\"Total sequences processed: {total_rows}\")\n\n        print(f\"Valid sequences: {valid_sequences}\")\n\n        print(f\"Skipped sequences: {skipped_sequences}\")\n\n        return sequences\n\n\n\n    def __len__(self):\n\n        return len(self.cascade_sequences)\n\n\n\n    def __getitem__(self, idx):\n\n        sequence, total_length = self.cascade_sequences[idx]\n\n        if len(sequence) > self.seq_length:\n\n            start_idx = np.random.randint(0, len(sequence) - self.seq_length)\n\n            sequence = sequence[start_idx:start_idx + self.seq_length]\n\n        return torch.FloatTensor(sequence), torch.FloatTensor([total_length])\n\n\n\nclass DeepCas(nn.Module):\n\n    \"\"\"\n\n    DeepCas implementation using Transformer architecture\n\n    \"\"\"\n\n    def __init__(self, input_size: int, d_model: int, nhead: int, num_layers: int):\n\n        super().__init__()\n\n        self.input_projection = nn.Linear(input_size, d_model)\n\n        self.positional_encoding = PositionalEncoding(d_model)\n\n        self.transformer_encoder = nn.TransformerEncoder(\n\n            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048),\n\n            num_layers\n\n        )\n\n        self.output_layer = nn.Sequential(\n\n            nn.Linear(d_model, d_model // 2),\n\n            nn.ReLU(),\n\n            nn.Dropout(0.1),\n\n            nn.Linear(d_model // 2, 1)\n\n        )\n\n\n\n    def forward(self, x):\n\n        x = self.input_projection(x)\n\n        x = self.positional_encoding(x)\n\n        x = self.transformer_encoder(x)\n\n        x = x.mean(dim=1)  # Global average pooling\n\n        return self.output_layer(x)\n\n\n\nclass DeepHawkes(nn.Module):\n\n    \"\"\"\n\n    DeepHawkes implementation with attention mechanism\n\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int):\n\n        super().__init__()\n\n        self.feature_encoder = nn.Sequential(\n\n            nn.Linear(input_size, hidden_size),\n\n            nn.LayerNorm(hidden_size),\n\n            nn.ReLU(),\n\n            nn.Dropout(0.1)\n\n        )\n\n\n\n        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n\n\n\n        self.intensity_network = nn.Sequential(\n\n            nn.Linear(hidden_size, hidden_size),\n\n            nn.ReLU(),\n\n            nn.Dropout(0.1),\n\n            nn.Linear(hidden_size, 1),\n\n            nn.Softplus()\n\n        )\n\n\n\n    def forward(self, x):\n\n        features = self.feature_encoder(x)\n\n        features = features.permute(1, 0, 2)  # [seq_len, batch, hidden]\n\n\n\n        attended_features, _ = self.attention(features, features, features)\n\n        attended_features = attended_features.permute(1, 0, 2)  # [batch, seq_len, hidden]\n\n\n\n        intensity = self.intensity_network(attended_features)\n\n        return intensity.sum(dim=1)  # Aggregate intensity over time\n\n\n\nclass CasCN(nn.Module):\n    \"\"\"\n    CasCN implementation with residual connections and dilated convolutions\n    Fixed to handle dimension matching properly\n    \"\"\"\n    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n        super().__init__()\n        self.input_proj = nn.Linear(input_size, hidden_size)\n        \n        # Dilated Causal Convolution blocks with padding adjustment\n        self.conv_blocks = nn.ModuleList()\n        self.residual_dense = nn.ModuleList()\n        \n        for i in range(num_layers):\n            # Add conv block\n            self.conv_blocks.append(\n                DilatedCausalConv1d(\n                    hidden_size,\n                    hidden_size,\n                    kernel_size=3,\n                    dilation=2**i\n                )\n            )\n            \n            # Add residual dense connection\n            self.residual_dense.append(\n                nn.Sequential(\n                    nn.Linear(hidden_size * (i + 1), hidden_size),\n                    nn.LayerNorm(hidden_size)\n                )\n            )\n        \n        self.output_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, 1)\n        )\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n        \n        # Initial projection\n        x = self.input_proj(x)  # [batch, seq_len, hidden]\n        \n        # Store features for dense connections\n        features = []\n        current = x\n        \n        for conv, dense in zip(self.conv_blocks, self.residual_dense):\n            features.append(current)\n            \n            # Combine features for dense connection\n            if len(features) > 1:\n                combined = torch.cat(features, dim=-1)\n                current = dense(combined)\n            else:\n                current = dense(current)\n            \n            # Apply convolution while maintaining sequence length\n            conv_out = conv(current)\n            \n            # Handle potential sequence length changes\n            if conv_out.size(1) != current.size(1):\n                # Adjust conv_out to match current's sequence length\n                conv_out = F.pad(conv_out, (0, 0, 0, current.size(1) - conv_out.size(1)))\n            \n            # Residual connection\n            current = F.relu(conv_out + current)\n        \n        # Global average pooling\n        pooled = current.mean(dim=1)\n        \n        return self.output_layer(pooled)\n\n\n\n\nclass TiDeH(nn.Module):\n    \"\"\"\n    TiDeH implementation with neural ODE approach and fixed dimension handling\n    \"\"\"\n    def __init__(self, input_size: int, hidden_size: int):\n        super().__init__()\n        self.hidden_size = hidden_size\n\n        # Feature encoder with layer normalization\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n\n        self.ode_func = ODEFunc(hidden_size)\n\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, 1),\n            nn.Softplus()\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, input_size]\n        batch_size, seq_len, _ = x.shape\n        \n        # Encode sequence\n        h = self.encoder(x)  # [batch_size, seq_len, hidden_size]\n        \n        # Process each timestep through ODE\n        # We'll use the last hidden state for prediction\n        h = h[:, -1, :]  # Take last timestep: [batch_size, hidden_size]\n        \n        # Create time points for ODE solving\n        t = torch.linspace(0, 1, steps=10).to(x.device)\n        \n        # Solve ODE\n        h = self.ode_solve(self.ode_func, h, t)  # [batch_size, hidden_size]\n        \n        # Decode to get final prediction\n        output = self.decoder(h)  # [batch_size, 1]\n        \n        return output\n\n    def ode_solve(self, func, y0, t):\n        \"\"\"\n        Simple Euler method for ODE solving with improved stability\n        Args:\n            func: ODEFunc instance\n            y0: Initial condition [batch_size, hidden_size]\n            t: Time points [steps]\n        \"\"\"\n        h = t[1] - t[0]\n        y = y0  # [batch_size, hidden_size]\n\n        for i in range(len(t) - 1):\n            slope = func(t[i], y)\n            y = y + h * slope\n            \n            # Add stability through normalization\n            y = F.layer_norm(y, [self.hidden_size])\n\n        return y\n\nclass ODEFunc(nn.Module):\n    \"\"\"ODE function for TiDeH with improved stability\"\"\"\n    def __init__(self, hidden_size: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Tanh(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size)\n        )\n\n    def forward(self, t, y):\n        return self.net(y)\n\n\n\nclass DilatedCausalConv1d(nn.Module):\n    \"\"\"\n    Dilated Causal Convolution layer with fixed sequence length handling\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, dilation: int = 1):\n        super().__init__()\n        self.causal_padding = (kernel_size - 1) * dilation\n        \n        # Use same padding to maintain sequence length\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding='same',  # This ensures output length matches input length\n            dilation=dilation\n        )\n        \n        self.layer_norm = nn.LayerNorm(out_channels)\n\n    def forward(self, x):\n        # Input shape: [batch, seq_len, channels]\n        batch_size, seq_len, channels = x.shape\n        \n        # Add causal padding at the start\n        x = F.pad(x.transpose(1, 2), (self.causal_padding, 0))\n        \n        # Apply convolution\n        x = self.conv(x)\n        \n        # Convert back and apply layer norm\n        x = x.transpose(1, 2)\n        x = self.layer_norm(x)\n        \n        return x[:, :seq_len, :]  # Ensure output length matches input length\n\n\n\nclass PositionalEncoding(nn.Module):\n\n    \"\"\"Positional encoding for transformer-based models\"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 5000):\n\n        super().__init__()\n\n        position = torch.arange(max_len).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n\n        pe = torch.zeros(max_len, 1, d_model)\n\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe)\n\n\n\n    def forward(self, x):\n\n        return x + self.pe[:x.size(0)]\n\n\n\ndef train_cascade_model(\n\n    model: nn.Module,\n\n    train_loader: DataLoader,\n\n    val_loader: DataLoader,\n\n    num_epochs: int = 50,\n\n    learning_rate: float = 0.001,\n\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n):\n\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n\n    criterion = nn.MSELoss()\n\n\n\n    best_val_loss = float('inf')\n\n    best_model = None\n\n\n\n    for epoch in range(num_epochs):\n\n        # Training\n\n        model.train()\n\n        train_loss = 0\n\n        for batch_x, batch_y in train_loader:\n\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n\n\n            optimizer.zero_grad()\n\n            output = model(batch_x)\n\n            loss = criterion(output, batch_y)\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n\n\n            train_loss += loss.item()\n\n\n\n        # Validation\n\n        model.eval()\n\n        val_loss = 0\n\n        with torch.no_grad():\n\n            for batch_x, batch_y in val_loader:\n\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n                output = model(batch_x)\n\n                val_loss += criterion(output, batch_y).item()\n\n\n\n        train_loss /= len(train_loader)\n\n        val_loss /= len(val_loader)\n\n\n\n        # Update learning rate\n\n        scheduler.step(val_loss)\n\n\n\n        # Save best model\n\n        if val_loss < best_val_loss:\n\n            best_val_loss = val_loss\n\n            best_model = model.state_dict().copy()\n\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}:')\n\n        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\n\n\n    # Load best model\n\n    model.load_state_dict(best_model)\n\n    return model\n\n\n\ndef main():\n\n    # Model hyperparameters\n\n    input_size = 2  # Since we have normalized_time and normalized_followers\n\n    hidden_size = 128\n\n    num_layers = 6\n\n    nhead = 8  # Number of attention heads for transformer\n\n    batch_size = 32\n\n    num_epochs = 50\n\n    learning_rate = 0.001\n\n\n\n    print(\"\\nInitializing dataset...\")\n\n    try:\n\n        dataset = TimeSeriesDataset('data.csv', 'index.csv')\n\n\n\n        if len(dataset) < 2:\n\n            raise ValueError(\"Dataset contains too few sequences for training/validation split\")\n\n\n\n        # Calculate split sizes\n\n        train_size = int(0.8 * len(dataset))\n\n        val_size = len(dataset) - train_size\n\n\n\n        # Create data splits\n\n        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n\n\n        print(f\"\\nDataset split:\")\n\n        print(f\"Total sequences: {len(dataset)}\")\n\n        print(f\"Training sequences: {len(train_dataset)}\")\n\n        print(f\"Validation sequences: {len(val_dataset)}\")\n\n\n\n        # Create data loaders\n\n        train_loader = DataLoader(\n\n            train_dataset,\n\n            batch_size=batch_size,\n\n            shuffle=True,\n\n            num_workers=0,  # Set to 0 for debugging\n\n            pin_memory=True if torch.cuda.is_available() else False\n\n        )\n\n\n\n        val_loader = DataLoader(\n\n            val_dataset,\n\n            batch_size=batch_size,\n\n            num_workers=0,  # Set to 0 for debugging\n\n            pin_memory=True if torch.cuda.is_available() else False\n\n        )\n\n\n\n        # Initialize models with proper parameters\n\n        print(\"\\nInitializing models...\")\n\n        models = {\n\n            'DeepCas': DeepCas(\n\n                input_size=input_size,\n\n                d_model=hidden_size,\n\n                nhead=nhead,\n\n                num_layers=num_layers\n\n            ),\n\n            'DeepHawkes': DeepHawkes(\n\n                input_size=input_size,\n\n                hidden_size=hidden_size\n\n            ),\n\n            'CasCN': CasCN(\n\n                input_size=input_size,\n\n                hidden_size=hidden_size,\n\n                num_layers=num_layers\n\n            ),\n            \n            'TiDeH': TiDeH(\n\n                input_size=input_size,\n\n                hidden_size=hidden_size\n            )\n\n        }\n\n\n\n        # Train each model\n\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        print(f\"\\nUsing device: {device}\")\n\n\n\n        results = {}\n\n        for name, model in models.items():\n\n            print(f\"\\nTraining {name}...\")\n\n            trained_model = train_cascade_model(\n\n                model=model,\n\n                train_loader=train_loader,\n\n                val_loader=val_loader,\n\n                num_epochs=num_epochs,\n\n                learning_rate=learning_rate,\n\n                device=device\n\n            )\n\n            results[name] = trained_model\n\n\n\n            # Save model checkpoint\n\n            torch.save({\n\n                'model_state_dict': trained_model.state_dict(),\n\n                'model_name': name,\n\n                'hyperparameters': {\n\n                    'input_size': input_size,\n\n                    'hidden_size': hidden_size,\n\n                    'num_layers': num_layers,\n\n                    'nhead': nhead\n\n                }\n\n            }, f'checkpoint_{name}.pt')\n\n\n\n        print(\"\\nTraining completed for all models!\")\n\n\n\n    except Exception as e:\n\n        print(f\"\\nError during training: {str(e)}\")\n\n        print(\"Stack trace:\")\n\n        import traceback\n\n        traceback.print_exc()\n\n        raise\n\n\n\nif __name__ == \"__main__\":\n\n    main()","metadata":{"id":"UaylXyLaLzZ1","outputId":"7185f975-bdd6-4c6b-f661-fcdce235a3d4","trusted":true,"execution":{"iopub.status.busy":"2024-11-04T19:47:38.235933Z","iopub.execute_input":"2024-11-04T19:47:38.236342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0PdlINlgL0Ws"},"outputs":[],"execution_count":null}]}